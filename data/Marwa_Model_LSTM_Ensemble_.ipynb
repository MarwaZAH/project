{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marwa_Model_LSTM_Ensemble .ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqLfv2HRqFHB"
      },
      "source": [
        "! pip install scikit-plot mlxtend\n",
        "! pip install -U scikit-learn\n",
        "#!python -c \"import sklearn; sklearn.show_versions()\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ksPHKiv_E2C"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jO0Hs5nkEE7"
      },
      "source": [
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import Model\n",
        "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Concatenate, Bidirectional, Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from time import time\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from time import time\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plot\n",
        "import seaborn\n",
        "from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "import sys\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0jfNVZ_ICO"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guEKM0lkg04r"
      },
      "source": [
        "def return_shape(X_trainz,window):\n",
        "  size=X_trainz.shape[0]#* X_trainz.shape[1]\n",
        "  wind=window\n",
        "  shape0=0\n",
        "  for i in range(2,window):\n",
        "    if size%i==0:\n",
        "      #print(i,278343/i)\n",
        "      x=size/i\n",
        "      for f in range(2,window):\n",
        "        if x%f==0 and x*f==size:\n",
        "          shape0=x\n",
        "          wind=f\n",
        "          #print(x,f, x/f,'\\n-------- \\n')\n",
        "  return int(shape0),wind\n",
        "\n",
        "def return_shape2(X_trainz,window):\n",
        "  size=X_trainz.shape[0]#* X_trainz.shape[1]\n",
        "  wind=window\n",
        "  shape0=0\n",
        "  for i in range(2,window):\n",
        "    if size%i==0:\n",
        "      #print(i,278343/i)\n",
        "      x=size/i\n",
        "      for f in range(2,window):\n",
        "        if x%f==0:\n",
        "          shape0=size/f\n",
        "          wind=f\n",
        "          #print(x,f, x/f,'\\n-------- \\n')\n",
        "  return int(shape0),wind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOoFkvA8tBE"
      },
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=4)\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "df  = pd.read_csv('/content/marwa.csv')\n",
        "# Training2 = pd.read_csv('SmartFall Training.csv')\n",
        "# df = pd.concat([Training2, Testing2], ignore_index=True)\n",
        "#df.describe()\n",
        "\n",
        "shape0,w1=return_shape2(df,25)\n",
        "#hape0,w1=return_shape(Training,25)\n",
        "print(df.shape,shape0,w1,shape0*w1,shape0*w1*3)\n",
        "#Training[['ms_accelerometer_x']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RudFE7Ma-uE"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRyCBqdibUUp"
      },
      "source": [
        "col=df.columns\n",
        "\n",
        "df=df[col[1:]]\n",
        "df[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTsNzAjrdXCT"
      },
      "source": [
        "df = df.replace(',', '', regex=True)\n",
        "df = df[1:].astype(float)\n",
        "print (df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9kv_jd6bc0_"
      },
      "source": [
        "# Old idea\n",
        "Train and test split from files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfY_UC72kLbN"
      },
      "source": [
        "# col=Training2.columns\n",
        "# total=int(Training2.shape[0]/w1)*w1\n",
        "# total1=int(Testing2.shape[0]/w1)*w1\n",
        "# Training=Training2.iloc[:total,:]\n",
        "# Testing=Testing2.iloc[:total1,:]\n",
        "\n",
        "# # split into train and test\n",
        "# lab_enc = preprocessing.LabelEncoder()\n",
        "# trainy = lab_enc.fit_transform(Training[col[-1]])\n",
        "# testy  = lab_enc.fit_transform(Testing[col[-1]])\n",
        "# #--------------------------------------------------------------------\n",
        "# n_train = 100\n",
        "# #y = to_categorical(y)\n",
        "# trainX, testX = Training[col[:-1]], Testing[col[:-1]]\n",
        "# #trainy, testy = to_categorical(Training[col[-1]]), to_categorical(Testing[col[-1]])\n",
        "\n",
        "# sc = StandardScaler()\n",
        "# trainX = sc.fit_transform(trainX)\n",
        "# testX  = sc.fit_transform(testX)\n",
        "\n",
        "# print(trainX.shape, testX.shape, trainy.shape ,testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFsjQfzfblGQ"
      },
      "source": [
        "# New idea \n",
        "- normal split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj4rYblTbp0H"
      },
      "source": [
        "col=df.columns\n",
        "X, y=df[col[1: ]], df[col[0]]\n",
        "X = preprocessing.minmax_scale(X)\n",
        "# split into train and test\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "y = lab_enc.fit_transform(y)\n",
        "\n",
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "print(trainX.shape, testX.shape, trainy.shape ,testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eViFitqV_K4K"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr2EhDdwkxSI"
      },
      "source": [
        "def build_model2(layers):\n",
        "        d = 0.2\n",
        "        #input_shape = X_train.shape\n",
        "        model = Sequential()\n",
        "        #Input_layer\n",
        "        model.add(LSTM(3, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "        #model.add(Dropout(d))\n",
        "        #LSTM_layer\n",
        "        model.add(LSTM(30, input_shape=(layers[0], layers[1]), return_sequences=False))\n",
        "        #model.add(Dropout(d))\n",
        "        #model.add(Flatten())\n",
        "        model.add(Dense(30, activation='relu'))        \n",
        "        #model.add(Flatten())\n",
        "        #Output_layer\n",
        "        model.add(Dense(2, activation='softmax' ))\n",
        "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "def build_model3(layers,Nueron,N_layer):\n",
        "        d = 0.2\n",
        "        #input_shape = X_train.shape\n",
        "        model = Sequential()\n",
        "\n",
        "        # The Input Layer :\n",
        "        model.add(LSTM(3, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "        #model.add(Dropout(d))\n",
        "        # The Hidden Layers :\n",
        "        for i in range(N_layer-1):\n",
        "          model.add(LSTM(Nueron, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "          #model.add(Dropout(d))\n",
        "        #model.add(Flatten())\n",
        "        model.add(LSTM(Nueron, input_shape=(layers[0], layers[1]), return_sequences=False))\n",
        "        model.add(Dense(Nueron, activation='relu'))        \n",
        "        #model.add(Flatten())\n",
        "\n",
        "        # The Output Layer :\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "        # Compile the network :\n",
        "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAddumN8_Ngs"
      },
      "source": [
        "#Setup the plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuowsuKO_RaY"
      },
      "source": [
        "get_ipython().magic('matplotlib inline')\n",
        "style = seaborn.axes_style(\"whitegrid\")\n",
        "style[\"axes.grid\"] = False\n",
        "seaborn.set_style(\"whitegrid\", style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50T-a1V9_c9D"
      },
      "source": [
        "# Evaluate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QeCpF1D-2oU"
      },
      "source": [
        "from sklearn.metrics import auc,confusion_matrix,precision_recall_curve,precision_score,recall_score,roc_curve \n",
        "from sklearn.metrics import accuracy_score,f1_score,fbeta_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "#from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplt\n",
        "\n",
        "def F_b(beta,recal,precision):\n",
        "  return (1+beta**2)*((recal*precision)/(recal+(beta**2)*precision))\n",
        "\n",
        "def plot_precision_recall_curve(model,testy2,yhat):\n",
        "  # calculate precision-recall curve\n",
        "  precision, recall, thresholds = precision_recall_curve(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  # calculate F1 score\n",
        "  f1 = f1_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  probas = model.predict_proba(X_test2, batch_size=64)\n",
        "  skplt.metrics.plot_precision_recall_curve(y_test, probas)\n",
        "  plt.show()\n",
        "  #disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
        "  #disp.ax_.set_title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "def plot_history(history,name_f):\n",
        "  plt.plot(history.history['accuracy'], label='training accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='testing accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()\n",
        "  \n",
        "  file_name=name_f+\"_Accuracy.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_history2(history,name_f):\n",
        "  plt.plot(history.history['loss'], label='training loss')\n",
        "  plt.plot(history.history['val_loss'], label='testing loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "  \n",
        "  file_name=name_f+\"_Loss.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_auc(model, testy2, yhat, name_f):\n",
        "  # Binarize the output\n",
        "  y = label_binarize(testy2, classes=[0, 1])\n",
        "  n_classes = y.shape[1]\n",
        "  y_test=testy2\n",
        "  y_score=yhat\n",
        "  # Compute ROC curve and ROC area for each class\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  # Compute micro-average ROC curve and ROC area\n",
        "  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
        "  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "  plt.figure()\n",
        "  lw = 2\n",
        "  plt.plot(fpr[1], tpr[1], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  file_name=name_f+\"_AUC_ROC.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def Evaluate_model(model,history,name_file,algo,time,score_table,X_test,y_test):\n",
        "  #X_train, y_train, X_test, y_test\n",
        "  testy2 = to_categorical(y_test)\n",
        "  loss, acc = model.evaluate(X_test, testy2, verbose=0)\n",
        "  print('Model Accuracy: %.3f' % acc)\n",
        "\n",
        "  # evaluate model on test set np.argmax(y_pred, axis=1)\n",
        "  yhat = model.predict(X_test)\n",
        "  acc = accuracy_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "   \n",
        "  precision, recall, f1, support = precision_recall_fscore_support(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  precision=np.average(precision, weights=support)\n",
        "  recall=np.average(recall, weights=support)\n",
        "  f1=np.average(f1, weights=support)\n",
        "  fb = fbeta_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1), average='micro', beta=3)\n",
        "\n",
        "  score_table.loc[algo,:] = time, acc, precision,recall,f1, F_b(3,recall,precision),fb\n",
        "  CM = confusion_matrix(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "  plt.show()\n",
        "\n",
        "  plot_precision_recall_curve(model,testy2,yhat)\n",
        "  plot_auc(model,testy2,yhat,name_file)\n",
        "  # print(history.history.keys())\n",
        "  plot_history(history,name_file)\n",
        "  plot_history2(history,name_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc257Umn_75I"
      },
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iI1mGECptiu"
      },
      "source": [
        "from numpy import array\n",
        "# (NumberOfExamples, TimeSteps, FeaturesPerStep)\n",
        "# X_train = train_inputs.reshape((split,3,2))\n",
        "# X_test = X_test.reshape((test_inputs.shape[0], 3, 2))\n",
        "# you have 3 steps and 2 features\n",
        "#TimeSteps\n",
        "window = 1\n",
        "X_train, y_train, X_test, y_test  = trainX, trainy, testX, testy\n",
        "\n",
        "X_train2 = np.reshape(array(X_train), (int(X_train.shape[0]/window), window, X_train.shape[1]))\n",
        "X_test2  = np.reshape(array(X_test), (int(X_test.shape[0]/window), window,  X_test.shape[1] ))\n",
        "\n",
        "y_train=y_train[:int(X_train.shape[0]/window)]\n",
        "y_test=y_test[:int(X_test.shape[0]/window)]\n",
        "#X_train2 = array(X_train).reshape( X_train.shape[0],1, X_train.shape[1])\n",
        "#X_test2 = array(X_test).reshape(X_test.shape[0],1, X_test.shape[1])\n",
        "X_train2.shape,y_train.shape, X_test2.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkbHoNq1Unkg"
      },
      "source": [
        "# Table to save final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkcH3YfoUn5k"
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(n) for n in [20,30,50,80]]\n",
        "score_table_2 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFrbuUpdACYX"
      },
      "source": [
        "# Models fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynNHwGZ20xeG"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "import time\n",
        "\n",
        "  \n",
        "def fit_model(window1,Nueron1,N_layer1,X_train_f,y_train_f,X_test_f,y_test_f,Name1,path_model,score_table):\n",
        "  layers=[window1,X_train_f.shape[2]]\n",
        "  Nueron=Nueron1\n",
        "  N_layer=N_layer1\n",
        "  start_time = time.clock()\n",
        "\n",
        "  model = build_model3(layers,Nueron,N_layer)\n",
        "  y_train2 = to_categorical(y_train_f)\n",
        "  print(\"X_train_f: \",X_train_f.shape)\n",
        "  history = model.fit(X_train_f,y_train2,batch_size=20, epochs=500,validation_split=0.1,verbose=0)\n",
        "  \n",
        "  # save the model\n",
        "  path=path_model\n",
        "  name_file=Name1+'_N_'+str(N_layer1)+'_L'\n",
        "  name_file1=os.path.join(path, name_file)\n",
        "  plot_model(model, show_shapes=True, to_file=name_file1+'_model.png')\n",
        "  model.save(name_file1+'_model.h5')\n",
        "  # summarize\n",
        "  print(model.summary())\n",
        "  time1 = time.clock() - start_time\n",
        "  Evaluate_model(model,history,name_file1,Name1,time1,score_table,X_test_f,y_test_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3U84koIU7Pe"
      },
      "source": [
        "# Results Summary- Single DL model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOprzPwjbQzk"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "  os.mkdir('Single DL model')\n",
        "  #os.mkdir('classification2')\n",
        "except:\n",
        "  shutil.rmtree('Single DL model')\n",
        "  #shutil.rmtree('classification2')\n",
        "  os.mkdir('Single DL model')\n",
        "  #os.mkdir('classification2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q98l0bmfEkwp"
      },
      "source": [
        "window1=1\n",
        "N_layer1=1\n",
        "path_model='./Single DL model/'\n",
        "for n in [90,80]:\n",
        "  Name1=\"LSTM_\"+str(n)\n",
        "  fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLUB_a2whfOf"
      },
      "source": [
        "score_table_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlaTkLnDVHot"
      },
      "source": [
        "# Results Summary- Single DL + additional layer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62U9Ous8YW6f"
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(n)+'| Added Layer' for n in [20,30,50,80]]\n",
        "score_table_3 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])\n",
        "\n",
        "try:\n",
        "  os.mkdir('Single additional layer model')\n",
        "except:\n",
        "  shutil.rmtree('Single additional layer model')\n",
        "  os.mkdir('Single additional layer model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbe73lnRXgwT"
      },
      "source": [
        "window1=1\n",
        "N_layer1=2\n",
        "path_model='./Single additional layer model/'\n",
        "for n in [20,30,50,80]:\n",
        "  Name1=\"LSTM_\"+str(n)+'| Added Layer'\n",
        "  fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUX-KlYFVIxd"
      },
      "source": [
        "score_table_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuV1ouWo4w5O"
      },
      "source": [
        "# Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXL5BTOFOjd4"
      },
      "source": [
        "# load all models\n",
        "#X_train2,y_train,X_test2,y_test\n",
        "n_members = 4\n",
        "path='/content/Single additional layer model/'\n",
        "members = load_all_models(n_members,path)\n",
        "print('Loaded %d models' % len(members))\n",
        "# evaluate standalone models on test dataset\n",
        "for model in members:\n",
        "\ttesty_enc = to_categorical(y_test)\n",
        "\t_, acc = model.evaluate(X_test2, testy_enc, verbose=0)\n",
        "\tprint('Model Accuracy: %.3f' % acc)\n",
        "# fit stacked model using the ensemble\n",
        "model,history = fit_stacked_model(members, X_test2, testy)\n",
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model, X_test2)\n",
        "acc = accuracy_score(testy, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfOWgcFnVEK2"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06EtvVK4fzz"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "bp = BaggingClassifier(model,n_estimators=10)\n",
        "bp=bp.fit(X_train,y_train)\n",
        "acc = bp.score(X_test, y_test)\n",
        "print('Model Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo66RxHPVXAF"
      },
      "source": [
        "# Results Summary- Ensemble DL with Boosting\n",
        "\n",
        "As we discussed, bagging is particularly effective for individual high-variance classifiers because the final majority-vote tends to smooth out the individual classifiers and produce a more stable collaborative solution. On the other hand, boosting is particularly effective for high-bias classifiers that are slow to adjust to new data. On the one hand, boosting is similiar to bagging in that it uses a majority-voting (or averaging for numeric prediction) process at the end; and it also combines individual classifiers of the same type. On the other hand, boosting is serially iterative, whereas the individual classifiers in bagging can be trained in parallel. Boosting uses the misclassifications of prior iterations to influence the training of the next iterative classifier by weighting those misclassifications more heavily in subsequent steps. This means that, at every step, boosting focuses more and more on specific misclassifications up to that point, letting the prior classifications be carried by earlier iterations.\n",
        "\n",
        "The primary implementation for boosting in Scikit-learn is the Adaptive Boosting (AdaBoost) algorithm, which does classification (AdaBoostClassifier) and regression (AdaBoostRegressor). The first step in the basic AdaBoost algorithm is to initialize the weights over each of the training set indicies, $D_0(i)=1/n$ where there are $n$ elements in the training set. Note that this creates a discrete uniform distribution over the indicies, not over the training data $\\lbrace (x_i,y_i) \\rbrace$ itself. In other words, if there are repeated elements in the training data, then each gets its own weight. The next step is to train the base classifer $h_k$ and record the classification error at the $k^{th}$ iteration, $\\epsilon_k$. Two factors can next be calculated using $\\epsilon_k$,\n",
        "$$ \\alpha_k = \\frac{1}{2}\\log \\frac{1-\\epsilon_k}{\\epsilon_k} $$\n",
        "\n",
        "and the normalization factor,\n",
        "$$ Z_k = 2 \\sqrt{ \\epsilon_k (1- \\epsilon_k) } $$\n",
        "\n",
        "For the next step, the weights over the training data are updated as in the following,\n",
        "$$ D_{k+1}(i) = \\frac{1}{Z_k} D_k(i)\\exp{(-\\alpha_k y_i h_k(x_i))} $$\n",
        "\n",
        "The final classification result is assembled using the $\\alpha_k$ factors, $g = \\sgn(\\sum_{k} \\alpha_k h_k)$.\n",
        "\n",
        "To re-do the problem above using boosting with perceptrons, we set up the AdaBoost classifier in the following,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG0YTlL_Zih7"
      },
      "source": [
        "- create models for stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Tk_lTbZbYr"
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(n)+'| Added Layer' for n in [20,30,50,80]]\n",
        "score_table_4 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])\n",
        "\n",
        "try:\n",
        "  os.mkdir('Ensemble DL with Boosting')\n",
        "except:\n",
        "  shutil.rmtree('Ensemble DL with Boosting')\n",
        "  os.mkdir('Ensemble DL with Boosting')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isn_ZANyZdiI"
      },
      "source": [
        "window1=1\n",
        "N_layer1=2\n",
        "path_model='./Ensemble DL with Boosting/'\n",
        "no_models=[10,15,50]\n",
        "for n in [5,10,15,30]:\n",
        "  Name1=str()+\"LSTM |\"+str(n)+'Neuron'\n",
        "  fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hZvhZJnVXV0"
      },
      "source": [
        "score_table_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UiDBOQqgGbV"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf=AdaBoostClassifier(model,n_estimators=3,\n",
        "                       algorithm='SAMME',\n",
        "                       learning_rate=0.5)\n",
        "clf=clf.fit(X_train,y_train)\n",
        "acc = clf.score(X_test, y_test)\n",
        "print('Model Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWLj7mseVj52"
      },
      "source": [
        "# Results Summary- Ensemble DL with Stacking \n",
        "\n",
        "# Stacking function\n",
        "- Load Model\n",
        "- Save Model\n",
        "- Stacked data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAI9SOuPdyXm"
      },
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# load models from file\n",
        "def load_all_models(n_models,path):\n",
        "  all_models = list()\n",
        "  names=['LSTM_'+str(n)+'| Added Layer_N_2_L' for n in [20,30,50,80]]\n",
        "  for i in range(n_models):\n",
        "    # define filename for this ensemble\n",
        "    filename = path+str(names[i])+'_model.h5'\n",
        "    # load model from file\n",
        "    model = load_model(filename)\n",
        "    # add to list of members\n",
        "    all_models.append(model)\n",
        "    print('>loaded %s' % filename)\n",
        "  return all_models\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "\tstackX = None\n",
        "\tfor model in members:\n",
        "\t\t# make prediction\n",
        "\t\tyhat = model.predict(inputX, verbose=0)\n",
        "\t\t# stack predictions into [rows, members, probabilities]\n",
        "\t\tif stackX is None:\n",
        "\t\t\tstackX = yhat\n",
        "\t\telse:\n",
        "\t\t\tstackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "\treturn stackX\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "# create new training data set from previous \n",
        "# fit standalone model # final model\n",
        "# eroor testy dim 2 --> dim 1 using inverse to_categorical\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\tmodel   = LogisticRegression()\n",
        "\tinputy2 = lab_enc.inverse_transform(inputy)#[np.argmax(inputy, axis=None, out=None) for y in inputy]\n",
        "\tprint('fit_stacked_model: ',stackedX.shape,inputy2.shape)\n",
        "\thistory = model.fit(stackedX, inputy2.reshape(-1,1))\n",
        "\treturn model,history\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "def Evaluate_model2(model,algo,time,score_table,X_test,testy2):\n",
        "   # evaluate model on test set np.argmax(y_pred, axis=1)\n",
        "  yhat = model.predict(X_test)\n",
        "  acc = accuracy_score(testy2, yhat)\n",
        "  recall1 = recall_score(testy2, yhat)\n",
        "  print('Model Test Accuracy: %.3f' % acc)\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(testy2, yhat)\n",
        "  #precision, recall, thresholds = precision_recall_curve(testy2, yhat)\n",
        "  # calculate F1 score \n",
        "  #f1 = f1_score(testy2, yhat)\n",
        "  precision=np.average(precision, weights=support)\n",
        "  recall=np.average(recall, weights=support)\n",
        "  f1=np.average(f1, weights=support)\n",
        "  fb = fbeta_score(testy2, yhat, average='micro', beta=3)\n",
        "\n",
        "  score_table.loc[algo,:] = time, acc, precision,recall,f1, F_b(3,recall,precision),fb\n",
        "  CM = confusion_matrix(testy2, yhat)\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcAVIVkddNa"
      },
      "source": [
        "- overlab data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjib2R35iml_"
      },
      "source": [
        "m='5'+'2'\n",
        "k=16\n",
        "z=2.5\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQHk7UFhdZfX"
      },
      "source": [
        "import math\n",
        "def assign_set2(no_models,m_overlap,list_models,start,end,data_size):\n",
        "  rows_per_model=round(data_size/((m_overlap-1)*(no_models-1)+1))#how many rows each model will take\n",
        "  total1 = data_size/no_models\n",
        "  total  = math.floor(total1)\n",
        "  #first model\n",
        "  list_models[start][0] = 0\n",
        "  list_models[end][0]   = total+m_overlap\n",
        "\n",
        "   #rest model\n",
        "  for i in range(1,no_models):\n",
        "     list_models[start][i] = total*i-(m_overlap)\n",
        "     list_models[end][i]   = list_models[start][i]+total+2*m_overlap\n",
        "   #last model\n",
        "  list_models[start][no_models-1] = total*(no_models-1)-m_overlap\n",
        "  list_models[end][no_models-1]   = data_size\n",
        "\n",
        "no_models=80\n",
        "m_overlap=5\n",
        "data_size=183806\n",
        "data_models=pd.DataFrame(index=range(no_models),\n",
        "                         columns=['start_row','end_row','start_overlap','end_overlap'])\n",
        "assign_set2(no_models,0,data_models,'start_row','end_row',data_size)\n",
        "assign_set2(no_models,m_overlap,data_models,'start_overlap','end_overlap',data_size)\n",
        "data_models[['start_overlap','end_overlap']].loc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdDAfKiWcnLY"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(m)+'M'+str(n)+'N' for m,n in zip([4,10,80],[30,15,6]) ]\n",
        "score_table_5 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])\n",
        "try:\n",
        "  os.mkdir('Ensemble Stacking')\n",
        "except:\n",
        "  shutil.rmtree('Ensemble Stacking')\n",
        "  os.mkdir('Ensemble Stacking')\n",
        "\n",
        "score_table_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWSmgv1_eEsN"
      },
      "source": [
        "- Create models for stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgDOThu5Vlqj"
      },
      "source": [
        "window1=1\n",
        "N_layer1=2\n",
        "no_models=[4,10,80]\n",
        "for n,j in zip([30,15,6],no_models):\n",
        "  Name1='LSTM_'+str(j)+'M'+str(n)+'N'\n",
        "  path_model='./Ensemble Stacking/'+Name1\n",
        "  os.mkdir(path_model)\n",
        "  for k in range(j):\n",
        "    Name1='LSTM_'+str(j)+'M'+str(n)+'N'+str(k)\n",
        "    fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p63DF8Qsz6pR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2GGeUBgeAxu"
      },
      "source": [
        "score_table_6=score_table_5\n",
        "# load all models\n",
        "no_models=[4,10,80]\n",
        "for n,j in zip([30,15,6],no_models):\n",
        "  Name1='LSTM_'+str(j)+'M'+str(n)+'N'\n",
        "  path_model='./Ensemble Stacking/'+Name1\n",
        "  #X_train2,y_train,X_test2,y_test\n",
        "  n_members = j\n",
        "  #path='/content/Single additional layer model/'\n",
        "  members = load_all_models(n_members,path_model)\n",
        "  print('Loaded %d models' % len(members))\n",
        "  # evaluate standalone models on test dataset\n",
        "  for model in members:\n",
        "    testy_enc = to_categorical(y_test)\n",
        "    _, acc = model.evaluate(X_test2, testy_enc, verbose=0)\n",
        "    print('Model Accuracy: %.3f' % acc)\n",
        "  # fit stacked model using the ensemble\n",
        "  model,history = fit_stacked_model(members, X_test2, testy)\n",
        "  # evaluate model on test set\n",
        "  yhat = stacked_prediction(members, model, X_test2)\n",
        "  acc = accuracy_score(testy, yhat)\n",
        "  print('Stacked Test Accuracy: %.3f' % acc)\n",
        "  Evaluate_model2(model,Name1,time,score_table_6,X_test,testy2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D52cgZtJcy0C"
      },
      "source": [
        "score_table_6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITsRFsHfLE4-"
      },
      "source": [
        "- download files from colab to local drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWfduiglLGuO"
      },
      "source": [
        "!zip -r /content/Ensemble Stacking.zip /content/Ensemble Stacking\n",
        "#!zip -r /content/classification2.zip /content/classification2\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/Ensemble Stacking.zip\")\n",
        "#files.download(\"/content/classification2.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZS2dAcTmfYP"
      },
      "source": [
        " - divide data based on Overlap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH0qHW3vmyVv"
      },
      "source": [
        "def overlab_data(no_models,m_overlap,data_size):\n",
        "  data_models=pd.DataFrame(index=range(no_models),\n",
        "                         columns=['start_row','end_row','start_overlap','end_overlap'])\n",
        "  assign_set2(no_models,0,data_models,'start_row','end_row',data_size)\n",
        "  assign_set2(no_models,m_overlap,data_models,'start_overlap','end_overlap',data_size)\n",
        "  return data_models#[['start_overlap','end_overlap']].loc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvB46gOEmdMS"
      },
      "source": [
        "window1=1\n",
        "N_layer1=2\n",
        "no_models=[4,10,80]\n",
        "for n,j in zip([30,15,6],no_models):\n",
        "  Name1='LSTM_'+str(j)+'M'+str(n)+'N'\n",
        "  path_model='./Ensemble Stacking Overlab/'+Name1\n",
        "  no_models=j\n",
        "  m_overlap=5\n",
        "  data_size=X_train2.shape[0]\n",
        "  od=overlab_data(no_models,m_overlap,data_size)\n",
        "  for k in range(j):\n",
        "    start,end=od[['start_overlap','end_overlap']].loc[k]\n",
        "    Name1='LSTM_'+str(j)+'M'+str(n)+'N'+str(k)\n",
        "    fit_model(window1,n,N_layer1,X_train2[start:end,:],y_train[start:end,:],X_test2,y_test,Name1,path_model,score_table_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA3bXDfuoVg3"
      },
      "source": [
        "score_table_7=score_table_5\n",
        "# load all models\n",
        "no_models=[4,10,80]\n",
        "for n,j in zip([30,15,6],no_models):\n",
        "  Name1='LSTM_'+str(j)+'M'+str(n)+'N'\n",
        "  path_model='./Ensemble Stacking/'+Name1\n",
        "  #X_train2,y_train,X_test2,y_test\n",
        "  n_members = j\n",
        "  #path='/content/Single additional layer model/'\n",
        "  members = load_all_models(n_members,path_model)\n",
        "  print('Loaded %d models' % len(members))\n",
        "  # evaluate standalone models on test dataset\n",
        "  for model in members:\n",
        "    testy_enc = to_categorical(y_test)\n",
        "    _, acc = model.evaluate(X_test2, testy_enc, verbose=0)\n",
        "    print('Model Accuracy: %.3f' % acc)\n",
        "  # fit stacked model using the ensemble\n",
        "  model,history = fit_stacked_model(members, X_test2, testy)\n",
        "  # evaluate model on test set\n",
        "  yhat = stacked_prediction(members, model, X_test2)\n",
        "  acc = accuracy_score(testy, yhat)\n",
        "  print('Stacked Test Accuracy: %.3f' % acc)\n",
        "  Evaluate_model2(model,Name1,time,score_table_6,X_test,testy2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfhjWgVISQT"
      },
      "source": [
        "#LSTM ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOr06-OsLN1j"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model,Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import Dense, Activation,Dropout,Conv1D,GlobalMaxPooling1D,GRU,Bidirectional,LSTM,SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "from keras import backend as K \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn import model_selection\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMUyrpFyInrM"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "  os.mkdir('LSTM ATTENTION')\n",
        "  #os.mkdir('classification2')\n",
        "except:\n",
        "  shutil.rmtree('LSTM ATTENTION')\n",
        "  #shutil.rmtree('classification2')\n",
        "  os.mkdir('LSTM ATTENTION')\n",
        "  #os.mkdir('classification2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyj2nMItITPx"
      },
      "source": [
        "def fit_model_L_A(layers,optimizer='adam'):\n",
        "  #input_shape = X_train.shape\n",
        "  model = Sequential()\n",
        "\n",
        "  # The Input Layer :\n",
        "  activations=LSTM(3, input_shape=(layers[0], layers[1]), return_sequences=True)\n",
        "  #model.add(Dropout(d))\n",
        "  # The Hidden Layers :\n",
        "  # compute importance for each step\n",
        "  attention = Dense(1, activation='tanh')(activations)\n",
        "  attention = keras.layers.Flatten()(attention)\n",
        "  attention = keras.layers.Activation('softmax')(attention)\n",
        "  attention = keras.layers.RepeatVector(units)(attention)\n",
        "  attention = keras.layers.Permute([2, 1])(attention)\n",
        "\n",
        "  multiply_layer = keras.layers.Multiply()\n",
        "  sent_representation = multiply_layer([activations, attention])\n",
        "  # sent_representation = keras.layers.Multiply([activations, attention])\n",
        "  sent_representation = keras.layers.Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n",
        "\n",
        "  probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
        "  model_L_A = Model(input=input_, output=probabilities)\n",
        "  model_L_A.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "\n",
        "  return model_L_A\n",
        "\n",
        "\n",
        "model_L_A = KerasClassifier(build_fn=fit_model_L_A,epochs=3,batch_size=64, verbose=2)\n",
        "  \n",
        "model_L_A_param_grid = {'optimizer':['adam']}\n",
        "\n",
        "model_L_A_grid = GridSearchCV(model_L_A,cv=KFold(n_splits=2),param_grid=model_L_A_param_grid,\n",
        "                    return_train_score=True, scoring=['accuracy'],refit='accuracy')\n",
        "\n",
        "model_L_A_summary = fit_model_L_A([3,1])\n",
        "print(model_L_A_summary.summary())\n",
        "grid_results = model_L_A_grid.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fsPu3uOJENa"
      },
      "source": [
        "\n",
        "- StackingClassifier\n",
        "- Running GridSearch Models\n",
        "- Overlap Technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxQq9xyFJDlx"
      },
      "source": [
        "model_BG_grid_results = model_BG_grid.fit(X_train1, y_train1)\n",
        "model_BL_grid_results = model_BL_grid.fit(X_train2, y_train2)\n",
        "model_BR_grid_results = model_BR_grid.fit(X_train3, y_train3)\n",
        "model_L_grid_results = model_L_grid.fit(X_train, y_train)\n",
        "model_SR_grid_results = model_SR_grid.fit(X_train, y_train)\n",
        "model_GRU_grid_results = model_GRU_grid.fit(X_train, y_train)\n",
        "model_L_A_grid_results = model_L_A_grid.fit(X_train, y_train)\n",
        "model_text_CNN_grid_results = model_text_CNN_grid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIsTP8IUKcih"
      },
      "source": [
        "clf1 = grid_results\n",
        "clf2 = RandomForestClassifier(random_state=1, n_estimators=300)\n",
        "clf3 = GaussianNB()\n",
        "# Logit will be used for stacking\n",
        "lr = LogisticRegression(solver='lbfgs')\n",
        "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, use_probas=True, cv=2)\n",
        "\n",
        "# Do CV\n",
        "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
        "                      ['grid_results', \n",
        "                       'Random Forest', \n",
        "                       'Naive Bayes',\n",
        "                       'StackingClassifier']):\n",
        "\n",
        "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=2, scoring='roc_auc')\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
        "\n",
        "# Fit on train data / predict on test data\n",
        "sclf_fit = sclf.fit(X_train, y_train)\n",
        "mypreds = sclf_fit.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_TcbB7VWJWj"
      },
      "source": [
        "# Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV9IdUwdj99W"
      },
      "source": [
        "from datetime import datetime\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        tmin, tsec = divmod((datetime.now() - start_time).total_seconds(), 60)\n",
        "        print(' Time taken: %i minutes and %s seconds.' %(tmin, round(tsec, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbwRnfITv0AW"
      },
      "source": [
        "from sklearn.metrics.classification import precision_recall_fscore_support\n",
        "def Evaluate_model2(model,algo,time,score_table,X_test,testy2):\n",
        "   # evaluate model on test set np.argmax(y_pred, axis=1)\n",
        "  yhat = model.predict(X_test)\n",
        "  acc = accuracy_score(testy2, yhat)\n",
        "  recall1 = recall_score(testy2, yhat)\n",
        "  print('Model Test Accuracy: %.3f' % acc)\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(testy2, yhat)\n",
        "  #precision, recall, thresholds = precision_recall_curve(testy2, yhat)\n",
        "  # calculate F1 score \n",
        "  #f1 = f1_score(testy2, yhat)\n",
        "  precision=np.average(precision, weights=support)\n",
        "  recall=np.average(recall, weights=support)\n",
        "  f1=np.average(f1, weights=support)\n",
        "  fb = fbeta_score(testy2, yhat, average='micro', beta=3)\n",
        "\n",
        "  score_table.loc[algo,:] = time, acc, precision,recall,f1, F_b(3,recall,precision),fb\n",
        "  CM = confusion_matrix(testy2, yhat)\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "  plt.show()\n",
        "\n",
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['xgb','SVC','AdaBoostClassifier','BaggingClassifier',\n",
        "         'RandomForestClassifier','StackingClassifier']\n",
        "score_table_ml = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wPWcfmukEZU"
      },
      "source": [
        "# fit model on dataset\n",
        "def fit_model(trainX, trainy):\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(25, input_dim=3, activation='relu'))\n",
        "\tmodel.add(Dense(2, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy, epochs=500, verbose=0)\n",
        "\treturn model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmDpqvFL3yCk"
      },
      "source": [
        "def ml(model,name_clf,X_test,y_test,score_table_ml):\n",
        "  start_time = datetime.now()\n",
        "  \n",
        "  model.fit(X_test,y_test)\n",
        "  time=datetime.now()-start_time\n",
        "  Evaluate_model2(model,name_clf,time,score_table_ml,X_test,y_test)\n",
        "  print(classification_report_imbalanced(y_test, model.predict(X_test)))\n",
        "\n",
        "xgb=XGBClassifier()\n",
        "#SVC1= SVC(kernel='rbf', C=1e3,random_state=42)\n",
        "#SVC1.fit(X_test,y_test)\n",
        "Baggingclf = BaggingClassifier(base_estimator=xgb,n_estimators=10, random_state=0)\n",
        "Adaclf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "rfc= RandomForestClassifier(n_estimators=100,random_state=42)\n",
        "#rfc.fit(X_train, y_train)\n",
        "\n",
        "estimators = [('rf',  RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "              ('Ada', AdaBoostClassifier(n_estimators=100, random_state=0))]\n",
        "\n",
        "stc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())   \n",
        "\n",
        "classifier = [xgb,  Adaclf, Baggingclf, rfc, stc]\n",
        "index = ['xgb', 'AdaBoostClassifier','BaggingClassifier','RandomForestClassifier','StackingClassifier']\n",
        "\n",
        "for m,name_clf in zip(classifier,index):\n",
        "  print('\\n ========'+name_clf+'================= \\n')\n",
        "  ml(m,name_clf,X_test,y_test,score_table_ml)\n",
        "  print('\\n ========================= \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFjFlV61i1Qk"
      },
      "source": [
        "get_ipython().magic('matplotlib inline')\n",
        "style = seaborn.axes_style(\"whitegrid\")\n",
        "style[\"axes.grid\"] = False\n",
        "seaborn.set_style(\"whitegrid\", style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5R4nvAa5mOZ"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "#xgb,  Adaclf, Baggingclf, rfc, stc\n",
        "xgb_disp  = plot_precision_recall_curve(xgb, X_test, y_test)\n",
        "Adaclf_disp  = plot_precision_recall_curve(Adaclf, X_test, y_test, ax=xgb_disp.ax_)\n",
        "Baggingclf_disp = plot_precision_recall_curve(Baggingclf, X_test, y_test, ax=Adaclf_disp.ax_)\n",
        "rfc_disp = plot_precision_recall_curve(rfc, X_test, y_test, ax=Baggingclf_disp.ax_)\n",
        "clf2_disp = plot_precision_recall_curve(stc, X_test, y_test, ax=rfc_disp.ax_)\n",
        "clf2_disp.figure_.suptitle(\"Precision-Recall comparison\")\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "#plt.title('Precision-Recall comparison')\n",
        "plt.legend(prop=dict(size=14))\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfMeEM6d2JFK"
      },
      "source": [
        "score_table_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5oSnJvAUuVK"
      },
      "source": [
        "# Binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVqJwGoQ5g77"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "#from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = timer(None)\n",
        "\n",
        "#X, y = make_classification(random_state=0)\n",
        "X_train, X_test, y_train, y_test = trainX, testX, trainy, testy \n",
        "\n",
        "svc = SVC(random_state=42)\n",
        "svc.fit(X_train, y_train)\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "clf1 = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "clf1.fit(X_train, y_train)\n",
        "\n",
        "estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "              ('svr', make_pipeline(StandardScaler(),SVC(random_state=42)))]\n",
        "\n",
        "clf2 = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())             \n",
        "\n",
        "clf2.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "y_score1 = svc.predict(X_test)\n",
        "y_score2 = rfc.predict(X_test)\n",
        "y_score3 = clf1.predict(X_test)\n",
        "y_score4 = clf2.predict(X_test)\n",
        "\n",
        "average_precision1 = average_precision_score(y_test, y_score1)\n",
        "average_precision3 = average_precision_score(y_test, y_score3)\n",
        "average_precision4 = average_precision_score(y_test, y_score4)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(average_precision1))\n",
        "print('Average precision-recall score: {0:0.2f}'.format(average_precision3))\n",
        "print('Average precision-recall score: {0:0.2f}'.format(average_precision4))\n",
        "\n",
        "timer(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTVop7346BQm"
      },
      "source": [
        "def plot_p_recall2(names,precision,recall,average_precision):\n",
        "  # setup plot details\n",
        "  colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
        "  plt.figure(figsize=(7, 8))\n",
        "  \n",
        "  lines = []\n",
        "  labels = []\n",
        "  for i, color in zip(range(len(names)), colors):\n",
        "      l, = plt.plot(recall[i]['micro'], precision[i]['micro'],marker='o', linestyle='dashed', color=color ,lw=2 )\n",
        "      lines.append(l)\n",
        "      labels.append('{0} (area = {1:0.2f})'.format(names[i], average_precision[i]['micro']))\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.subplots_adjust(bottom=0.25)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.title('Precision-Recall comparison')\n",
        "  plt.legend(lines, labels, prop=dict(size=14))\n",
        "  plt.show()\n",
        "\n",
        "precision=[]\n",
        "recall=[]\n",
        "average_precision=[]\n",
        "for i in [y_score2,y_score3,y_score4]:\n",
        "  precision1,recall1,average_precision1=Pre_Recall(i)\n",
        "  precision.append(precision1)\n",
        "  recall.append(recall1)\n",
        "  average_precision.append(average_precision1)\n",
        "\n",
        "plot_p_recall2(names,precision,recall,average_precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4En7b866zzY"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "\n",
        "svc_disp  = plot_precision_recall_curve(svc, X_test, y_test)\n",
        "rfc_disp  = plot_precision_recall_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\n",
        "clf1_disp = plot_precision_recall_curve(clf1, X_test, y_test, ax=rfc_disp.ax_)\n",
        "clf2_disp = plot_precision_recall_curve(clf2, X_test, y_test, ax=clf1_disp.ax_)\n",
        "clf2_disp.figure_.suptitle(\"Precision-Recall comparison\")\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "#plt.title('Precision-Recall comparison')\n",
        "plt.legend(prop=dict(size=14))\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2PwpBQU0GD"
      },
      "source": [
        "# Multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afFWxhNbCgkZ"
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "start_time = timer(None)\n",
        "\n",
        "# Use label_binarize to be multi-label like settings\n",
        "Y = label_binarize(y, classes=[0, 1, 2])\n",
        "n_classes = Y.shape[1]\n",
        "\n",
        "# Split into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.5,random_state=7)\n",
        "\n",
        "# We use OneVsRestClassifier for multi-label prediction\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Run classifier\n",
        "# SVC = SVC(random_state=7)\n",
        "# SVC.fit(X_train, y_train)\n",
        "# y_score1 = SVC.decision_function(X_test)\n",
        "\n",
        "rfc = OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
        "rfc.fit(X_train, y_train)\n",
        "y_score2 = rfc.predict(X_test)\n",
        "\n",
        "clf1 = OneVsRestClassifier(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
        "clf1.fit(X_train, y_train)\n",
        "y_score3 = clf1.predict(X_test)\n",
        "\n",
        "estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "              ('rf1', RandomForestClassifier(n_estimators=20, random_state=42))]\n",
        "\n",
        "clf2 = OneVsRestClassifier(StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()))\n",
        "clf2.fit(X_train, y_train).score(X_test, y_test)\n",
        "y_score4 = clf2.predict(X_test)\n",
        "\n",
        "names=['RandomForestClassifier','AdaBoostClassifier','StackingClassifier']\n",
        "\n",
        "timer(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Si_rHkuBy64"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "\n",
        "def Pre_Recall(y_score1):\n",
        "  # For each class\n",
        "  precision = dict()\n",
        "  recall = dict()\n",
        "  average_precision = dict()\n",
        "  for i in range(n_classes):\n",
        "      precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],y_score1[:, i])\n",
        "      average_precision[i] = average_precision_score(y_test[:, i], y_score1[:, i])\n",
        "\n",
        "  # A \"micro-average\": quantifying score on all classes jointly\n",
        "  precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),y_score1.ravel())\n",
        "  average_precision[\"micro\"] = average_precision_score(y_test, y_score1, average=\"micro\")\n",
        "  print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "  return precision,recall,average_precision\n",
        "\n",
        "def plot_p_recall2(names,precision,recall,average_precision):\n",
        "  # setup plot details\n",
        "  colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
        "  plt.figure(figsize=(7, 8))\n",
        "  \n",
        "  lines = []\n",
        "  labels = []\n",
        "  for i, color in zip(range(len(names)), colors):\n",
        "      l, = plt.plot(recall[i]['micro'], precision[i]['micro'],marker='o', linestyle='dashed', color=color ,lw=2 )\n",
        "      lines.append(l)\n",
        "      labels.append('{0} (area = {1:0.2f})'.format(names[i], average_precision[i]['micro']))\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.subplots_adjust(bottom=0.25)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.title('Precision-Recall comparison')\n",
        "  plt.legend(lines, labels, prop=dict(size=14))\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThLqMN1WKXa2"
      },
      "source": [
        "precision=[]\n",
        "recall=[]\n",
        "average_precision=[]\n",
        "for i in [y_score2,y_score3,y_score4]:\n",
        "  precision1,recall1,average_precision1=Pre_Recall(i)\n",
        "  precision.append(precision1)\n",
        "  recall.append(recall1)\n",
        "  average_precision.append(average_precision1)\n",
        "\n",
        "plot_p_recall2(names,precision,recall,average_precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlVI1YfbKa37"
      },
      "source": [
        "average_precision#[0]['micro']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQ5qMfKmUR9"
      },
      "source": [
        "#Comparison of ensembling classifiers internally using sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqsl-Z6aposo"
      },
      "source": [
        "from collections import Counter\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "\n",
        "print('Training target statistics: {}'.format(Counter(y_train)))\n",
        "print('Testing target statistics: {}'.format(Counter(y_test)))\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=-1)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Classify and report the results\n",
        "print(classification_report_imbalanced(y_test, rf.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVAWOdUomU-4"
      },
      "source": [
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "#from imblearn.datasets import fetch_datasets\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.ensemble import EasyEnsembleClassifier\n",
        "from imblearn.ensemble import RUSBoostClassifier\n",
        "\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, ax, normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    print(cm)\n",
        "    print('')\n",
        "\n",
        "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.set_title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.sca(ax)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-btGWwjnJ-n"
      },
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "print('Decision tree classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_tree),geometric_mean_score(y_test, y_pred_tree)))\n",
        "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
        "fig, ax = plt.subplots()\n",
        "plot_confusion_matrix(cm_tree, classes=np.unique([0,1]), ax=ax, title='Decision tree')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tKQD-wKngAn"
      },
      "source": [
        "bagging = BaggingClassifier(n_estimators=50, random_state=0, n_jobs=-1)\n",
        "balanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0,n_jobs=-1)\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "alanced_bagging.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bc = bagging.predict(X_test)\n",
        "y_pred_bbc = balanced_bagging.predict(X_test)\n",
        "\n",
        "print('Bagging classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_bc),geometric_mean_score(y_test, y_pred_bc)))\n",
        "cm_bagging = confusion_matrix(y_test, y_pred_bc)\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "plot_confusion_matrix(cm_bagging, classes=np.unique([0,1]), ax=ax[0], title='Bagging')\n",
        "\n",
        "print('Balanced Bagging classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_bbc), geometric_mean_score(y_test, y_pred_bbc)))\n",
        "cm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\n",
        "plot_confusion_matrix(cm_balanced_bagging, classes=np.unique([0,1]),ax=ax[1], title='Balanced bagging')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RXJLFPWoGtN"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=-1)\n",
        "brf = BalancedRandomForestClassifier(n_estimators=50, random_state=0,\n",
        "                                     n_jobs=-1)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "brf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_brf = brf.predict(X_test)\n",
        "\n",
        "# Similarly to the previous experiment, the balanced classifier outperform the\n",
        "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
        "# forest outsperforms the bagging classifier.\n",
        "\n",
        "print('Random Forest classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_rf),\n",
        "              geometric_mean_score(y_test, y_pred_rf)))\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "plot_confusion_matrix(cm_rf, classes=np.unique([0,1]), ax=ax[0],\n",
        "                      title='Random forest')\n",
        "\n",
        "print('Balanced Random Forest classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_brf),\n",
        "              geometric_mean_score(y_test, y_pred_brf)))\n",
        "cm_brf = confusion_matrix(y_test, y_pred_brf)\n",
        "plot_confusion_matrix(cm_brf, classes=np.unique([0,1]), ax=ax[1],\n",
        "                      title='Balanced random forest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acTnS8juoVU3"
      },
      "source": [
        "base_estimator = AdaBoostClassifier(n_estimators=10)\n",
        "eec = EasyEnsembleClassifier(n_estimators=10,base_estimator=base_estimator)\n",
        "eec.fit(X_train, y_train)\n",
        "y_pred_eec = eec.predict(X_test)\n",
        "print('Easy ensemble classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_eec), geometric_mean_score(y_test, y_pred_eec)))\n",
        "cm_eec = confusion_matrix(y_test, y_pred_eec)\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "plot_confusion_matrix(cm_eec, classes=np.unique([0,1]), ax=ax[0],title='Easy ensemble classifier')\n",
        "\n",
        "rusboost = RUSBoostClassifier(n_estimators=10, base_estimator=base_estimator)\n",
        "rusboost.fit(X_train, y_train)\n",
        "y_pred_rusboost = rusboost.predict(X_test)\n",
        "print('RUSBoost classifier performance:')\n",
        "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "      .format(balanced_accuracy_score(y_test, y_pred_rusboost),geometric_mean_score(y_test, y_pred_rusboost)))\n",
        "cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\n",
        "plot_confusion_matrix(cm_rusboost, classes=np.unique([0,1]),ax=ax[1], title='RUSBoost classifier')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}